{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Theoretical**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1 Can we use Bagging for regression problems?\n",
        "-Yes, Bagging (Bootstrap Aggregating) can absolutely be used for regression problems‚Äîand it's actually quite effective in many cases!\n",
        "\n",
        "\n",
        "\n",
        "2 What is the difference between multiple model training and single model training?\n",
        "-Great question! The difference between **multiple model training** and **single model training** is foundational to understanding how ensemble methods like **bagging, boosting**, or **stacking** work.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† **Single Model Training**\n",
        "This is the traditional approach:\n",
        "\n",
        "- You train **one model** (e.g., a decision tree, linear regression, neural network) on the entire training dataset.\n",
        "- This model is solely responsible for making predictions.\n",
        "\n",
        "#### ‚úÖ Pros:\n",
        "- Simpler to implement and understand.\n",
        "- Faster to train and evaluate.\n",
        "- Requires fewer resources.\n",
        "\n",
        "#### ‚ùå Cons:\n",
        "- If the model is high variance (like decision trees), it might overfit.\n",
        "- If it's too simple, it might underfit.\n",
        "- Less robust to noise or data quirks.\n",
        "\n",
        "---\n",
        "\n",
        "### ü§ñ **Multiple Model Training (Ensemble Learning)**\n",
        "Here, you train **multiple models** (often of the same type) and combine their predictions.\n",
        "\n",
        "**Two popular forms:**\n",
        "1. **Bagging** (e.g., Random Forest): Models trained on different random subsets of data.\n",
        "2. **Boosting** (e.g., XGBoost, AdaBoost): Models trained sequentially, each correcting the previous.\n",
        "\n",
        "#### ‚úÖ Pros:\n",
        "- **Reduces variance and/or bias.**\n",
        "- More accurate and robust.\n",
        "- Handles complex data better.\n",
        "\n",
        "#### ‚ùå Cons:\n",
        "- More computationally expensive.\n",
        "- Harder to interpret.\n",
        "- Slower to train and predict.\n",
        "\n",
        "---\n",
        "\n",
        "### üîç Quick Comparison Table\n",
        "\n",
        "| Feature                  | Single Model             | Multiple Model (Ensemble)     |\n",
        "|--------------------------|--------------------------|-------------------------------|\n",
        "| Number of Models         | 1                        | Many                          |\n",
        "| Prediction               | From one model           | Combined from many models     |\n",
        "| Complexity               | Simpler                  | More complex                  |\n",
        "| Accuracy (often)         | Lower                    | Higher                        |\n",
        "| Overfitting Risk         | Higher (for complex models) | Lower (especially in bagging) |\n",
        "| Interpretability         | Easier                   | Harder                        |\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "3 Explain the concept of feature randomness in Random Forest2\n",
        "-Normally, when a decision tree splits a node, it looks through all available features and chooses the best one to split on.\n",
        "\n",
        "But in a Random Forest, when building each tree:\n",
        "\n",
        "At each split, the algorithm considers only a random subset of the features, not all of them.\n",
        "\n",
        "From that subset, it picks the best feature to split the node.\n",
        "\n",
        "üîß Example:\n",
        "Suppose you have a dataset with 10 features, and you're growing a tree in a Random Forest.\n",
        "\n",
        "At each split, instead of checking all 10 features, the algorithm might randomly select, say, 3 features, and only evaluate those.\n",
        "\n",
        "This random selection is repeated at every split in the tree.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "4 What is OOB (Out-of-Bag) Score?\n",
        "-In Bagging (Bootstrap Aggregating), each base model (like a decision tree) is trained on a bootstrap sample‚Äîa random sample with replacement from the training data.\n",
        "\n",
        "Because of this sampling method:\n",
        "\n",
        "Some training instances are selected multiple times, and\n",
        "\n",
        "Some are not selected at all for a particular tree.\n",
        "\n",
        "\n",
        "5  How can you measure the importance of features in a Random Forest model?\n",
        "-Awesome question! Measuring **feature importance** in a **Random Forest** helps us understand which features are contributing the most to the model‚Äôs predictions. There are a couple of common ways to do this:\n",
        "\n",
        "---\n",
        "\n",
        "## üå≥ 1. **Mean Decrease in Impurity (MDI)** ‚Äì a.k.a. Gini Importance\n",
        "\n",
        "This is the **default method** in libraries like `scikit-learn`.\n",
        "\n",
        "### üîç How it works:\n",
        "- Every time a feature is used to split a node in any of the trees, the **impurity (e.g., Gini or MSE)** is reduced.\n",
        "- The reduction in impurity is **accumulated** for each feature across all trees.\n",
        "- The average total reduction per feature is then normalized ‚Üí **feature importance score**.\n",
        "\n",
        "### üß† Think of it as:\n",
        "> \"How much does this feature help reduce uncertainty (impurity) when splitting the data?\"\n",
        "\n",
        "### ‚úÖ Pros:\n",
        "- Fast and built-in\n",
        "- Works well for many cases\n",
        "\n",
        "### ‚ùå Cons:\n",
        "- Biased toward features with more categories or higher variance\n",
        "\n",
        "---\n",
        "\n",
        "## üé≤ 2. **Permutation Importance** (a.k.a. Mean Decrease in Accuracy)\n",
        "\n",
        "This is a **model-agnostic** method, often considered more reliable.\n",
        "\n",
        "### üîç How it works:\n",
        "1. Evaluate the model's performance (e.g., accuracy or R¬≤) on a validation or OOB set.\n",
        "2. **Randomly shuffle** the values of one feature across the dataset.\n",
        "3. Re-evaluate the model performance.\n",
        "4. The **drop in performance** tells you how important that feature was.\n",
        "\n",
        "### ‚úÖ Pros:\n",
        "- Less biased\n",
        "- Can work with any model, not just Random Forest\n",
        "\n",
        "### ‚ùå Cons:\n",
        "- Slower (requires retraining or reevaluation multiple times)\n",
        "\n",
        "---\n",
        "\n",
        "## üîß Code Example (Scikit-learn: MDI)\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "importances = model.feature_importances_\n",
        "feature_names = X_train.columns\n",
        "\n",
        "# Plot\n",
        "pd.Series(importances, index=feature_names).sort_values().plot(kind='barh')\n",
        "plt.title(\"Feature Importances\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üîß Code Example (Scikit-learn: Permutation Importance)\n",
        "\n",
        "```python\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "result = permutation_importance(model, X_val, y_val, n_repeats=10, random_state=42)\n",
        "importances = pd.Series(result.importances_mean, index=X_val.columns)\n",
        "importances.sort_values().plot(kind=\"barh\")\n",
        "plt.title(\"Permutation Feature Importances\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Summary\n",
        "\n",
        "| Method                  | Description                                       | Bias?           | Speed |\n",
        "|-------------------------|---------------------------------------------------|------------------|--------|\n",
        "| MDI (Gini Importance)   | Total impurity reduction from splits              | Biased (slightly)| Fast   |\n",
        "| Permutation Importance  | Measures drop in accuracy after shuffling a feature | Less biased     | Slower |\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6 Explain the working principle of a Bagging Classifier\n",
        "-üîß Working Principle: Step-by-Step\n",
        "Let‚Äôs say you‚Äôre training a Bagging Classifier on a labeled dataset.\n",
        "\n",
        "1. Bootstrap Sampling\n",
        "Create multiple random subsets of your training data.\n",
        "\n",
        "Each subset is created by sampling with replacement from the original dataset.\n",
        "\n",
        "Each subset is the same size as the original dataset (but with repeated instances).\n",
        "\n",
        "Example: If you have 100 training samples, each model gets a random 100-sample subset (some duplicates, some left out).\n",
        "\n",
        "2. Train Base Learners\n",
        "Train a separate model (usually of the same type, like decision trees) on each bootstrap sample.\n",
        "\n",
        "These models are called base estimators.\n",
        "\n",
        "3. Aggregate Predictions\n",
        "During prediction, each base model makes a prediction on the new (unseen) data.\n",
        "\n",
        "For classification: Use majority voting to decide the final class label.\n",
        "\n",
        "For regression (in Bagging Regressor): Take the average of all outputs.\n",
        "\n",
        "4. (Optional) Out-of-Bag (OOB) Evaluation\n",
        "You can use the data not included in each bootstrap sample (about 1/3 of the data) to get an unbiased estimate of model performance‚Äîthis is called the OOB score.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "7 How do you evaluate a Bagging Classifier‚Äôs performance?\n",
        "-Great question! Evaluating a **Bagging Classifier‚Äôs performance** is similar to how you'd evaluate any classification model, but there are also some **special tools unique to bagging** that can give you deeper insight‚Äîlike the **OOB (Out-of-Bag) score**.\n",
        "\n",
        "Let‚Äôs break it down üëá\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ **1. Standard Evaluation Metrics**\n",
        "Use these on your **test set** or through **cross-validation**:\n",
        "\n",
        "### üìä Classification Metrics:\n",
        "- **Accuracy** ‚Äì overall correct predictions\n",
        "- **Precision / Recall / F1-score** ‚Äì good for imbalanced classes\n",
        "- **Confusion Matrix** ‚Äì shows breakdown of TP, FP, FN, TN\n",
        "- **ROC AUC Score** ‚Äì good for binary classifiers, especially with imbalance\n",
        "- **Log Loss** ‚Äì evaluates predicted probabilities\n",
        "\n",
        "### Example (scikit-learn):\n",
        "```python\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "y_pred = bagging.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ **2. Out-of-Bag (OOB) Score** ‚Äì Bagging Bonus!\n",
        "\n",
        "The OOB score is a **built-in validation technique** available with Bagging.\n",
        "\n",
        "### üîç How it works:\n",
        "- Each base model is trained on a bootstrap sample.\n",
        "- About **1/3 of the data is left out** of each sample.\n",
        "- For each data point, you **average predictions** from only the models that **didn't train on it**.\n",
        "- This gives a reliable internal accuracy estimate‚Äîno need for separate validation set.\n",
        "\n",
        "### How to Use:\n",
        "```python\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "model = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=100,\n",
        "    oob_score=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "print(\"OOB Score:\", model.oob_score_)  # This is like validation accuracy\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ **3. Cross-Validation (Optional)**\n",
        "\n",
        "You can also use **k-fold cross-validation** for more reliable performance estimates:\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(model, X_train, y_train, cv=5)\n",
        "print(\"Cross-validation Accuracy: \", scores.mean())\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üìå Summary Table\n",
        "\n",
        "| Evaluation Method   | Purpose                                       | Notes                         |\n",
        "|---------------------|-----------------------------------------------|-------------------------------|\n",
        "| Accuracy / F1       | Basic classification performance              | Use test set or CV            |\n",
        "| ROC AUC / Log Loss  | For probabilistic output & imbalance          | Use `predict_proba()`         |\n",
        "| Confusion Matrix    | Visual error breakdown                        | Helpful for interpretation    |\n",
        "| OOB Score           | Internal validation (unique to Bagging)       | No separate test set needed   |\n",
        "| Cross-Validation    | More robust estimate over multiple splits     | Slower but reliable           |\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "8 How does a Bagging Regressor work?\n",
        "-1. üß∫ Bootstrap Sampling\n",
        "Create multiple bootstrap samples from the original training data.\n",
        "\n",
        "Each sample is created by random sampling with replacement.\n",
        "\n",
        "Each sample is typically the same size as the original dataset.\n",
        "\n",
        "2. üå≥ Train Base Regressors\n",
        "Train a separate regressor (e.g., DecisionTreeRegressor) on each bootstrap sample.\n",
        "\n",
        "All base models are trained independently.\n",
        "\n",
        "3. üìà Make Predictions\n",
        "For a new (unseen) data point:\n",
        "\n",
        "Each model makes its own prediction.\n",
        "\n",
        "The final output is the average of all individual predictions.\n",
        "\n",
        "üéØ Prediction = Mean of base regressor outputs\n",
        "\n",
        "4. üß™ (Optional) Out-of-Bag (OOB) Evaluation\n",
        "For each data point, you can evaluate its prediction using only the models that did not train on it.\n",
        "\n",
        "The OOB R¬≤ score can serve as an internal validation metric.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "9 What is the main advantage of ensemble techniques?\n",
        "-Improved performance (accuracy, robustness, and generalization) by reducing bias, variance, or both.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "10 What is the main challenge of ensemble methods?\n",
        "-üìå Top Challenges of Ensemble Methods:\n",
        "Challenge\tDescription\n",
        "üß† Interpretability\tHarder to explain predictions, especially with many base learners. (e.g., Random Forest vs a single decision tree)\n",
        "‚öôÔ∏è Training Time\tMultiple models = more compute time and memory usage.\n",
        "üöß Model Size & Deployment\tEnsembles can be large and slow to predict in real-time.\n",
        "üîç Hyperparameter Tuning\tMany parameters to set (number of estimators, learning rate, max depth, etc.)\n",
        "‚öñÔ∏è Overfitting (especially Boosting)\tIf not tuned well, ensembles can still overfit, especially in noisy datasets.\n",
        "üîÅ Data Leakage Risk (if misused)\tComplex workflows make it easier to accidentally leak test info into training.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "11 Explain the key idea behind ensemble techniques\n",
        "-‚úÖ Combine multiple individual models (\"weak learners\") to create a single, stronger predictive model.\n",
        "\n",
        "This strategy improves performance by reducing errors like bias, variance, or both.\n",
        "\n",
        "\n",
        "\n",
        "12 What is a Random Forest Classifier?\n",
        "-A Random Forest Classifier is an ensemble learning algorithm used for classification tasks. It builds a ‚Äúforest‚Äù of decision trees and combines their outputs to make more accurate, stable, and robust predictions than a single tree would.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "13 What are the main types of ensemble techniques?\n",
        "-Great question! There are **three main types** of ensemble techniques, each with its own strategy for combining models to improve performance:\n",
        "\n",
        "---\n",
        "\n",
        "## üîß 1. **Bagging (Bootstrap Aggregating)**\n",
        "\n",
        "### üìå Key Idea:\n",
        "> Train multiple models **in parallel** on different **random subsets** of the data, then **combine their outputs** (e.g., by voting or averaging).\n",
        "\n",
        "### üî• Famous Example:\n",
        "- **Random Forest** (uses bagging with decision trees)\n",
        "\n",
        "### ‚úÖ Strengths:\n",
        "- Reduces **variance**\n",
        "- Helps prevent **overfitting**\n",
        "- Works well with **high-variance models** (e.g., decision trees)\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ 2. **Boosting**\n",
        "\n",
        "### üìå Key Idea:\n",
        "> Train models **sequentially**, where each new model **focuses on correcting the errors** made by the previous ones.\n",
        "\n",
        "### üî• Famous Examples:\n",
        "- **AdaBoost**\n",
        "- **Gradient Boosting**\n",
        "- **XGBoost**, **LightGBM**, **CatBoost** (high-performance versions)\n",
        "\n",
        "### ‚úÖ Strengths:\n",
        "- Reduces **bias**\n",
        "- Builds a strong model from many **weak learners**\n",
        "- Great for **structured/tabular data**\n",
        "\n",
        "---\n",
        "\n",
        "## üß† 3. **Stacking (Stacked Generalization)**\n",
        "\n",
        "### üìå Key Idea:\n",
        "> Combine **different types of models** (e.g., decision trees, SVMs, logistic regression) and use a **meta-model** to learn how to best combine their predictions.\n",
        "\n",
        "### üî• How it works:\n",
        "- Base models make predictions.\n",
        "- A **meta-model** (like logistic regression) is trained on those predictions to make the final decision.\n",
        "\n",
        "### ‚úÖ Strengths:\n",
        "- Very **flexible**\n",
        "- Can combine the **strengths of multiple algorithms**\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Summary Table\n",
        "\n",
        "| Ensemble Type | Combines Models How?         | Goal              | Example                   |\n",
        "|----------------|------------------------------|-------------------|---------------------------|\n",
        "| **Bagging**     | Parallel voting/averaging    | ‚Üì Variance        | Random Forest             |\n",
        "| **Boosting**    | Sequential error correction  | ‚Üì Bias            | AdaBoost, XGBoost         |\n",
        "| **Stacking**    | Meta-model on base outputs   | ‚Üë Flexibility     | Any combo of models       |\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "14 What is ensemble learning in machine learning?\n",
        "-Great foundational question! Let's break it down nice and clear:\n",
        "\n",
        "---\n",
        "\n",
        "## üß† What is **Ensemble Learning** in Machine Learning?\n",
        "\n",
        "> **Ensemble learning** is a technique where **multiple models (learners)** are trained and combined to **solve the same problem** and produce **better results** than any single model alone.\n",
        "\n",
        "It‚Äôs like **‚Äúwisdom of the crowd‚Äù** ‚Äî many models working together are often smarter and more accurate than just one.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Why Use Ensemble Learning?\n",
        "\n",
        "- ‚úÖ To **increase accuracy**\n",
        "- ‚úÖ To **reduce overfitting or underfitting**\n",
        "- ‚úÖ To make models more **stable and robust**\n",
        "- ‚úÖ To **combine strengths** of different algorithms\n",
        "\n",
        "---\n",
        "\n",
        "## üß∞ Common Types of Ensemble Learning\n",
        "\n",
        "| Type       | Description                                      | Goal         |\n",
        "|------------|--------------------------------------------------|--------------|\n",
        "| **Bagging**| Train models in **parallel** on random data subsets | Reduce **variance** |\n",
        "| **Boosting**| Train models **sequentially**, focusing on errors | Reduce **bias** |\n",
        "| **Stacking**| Combine **different model types** with a meta-model | Improve **flexibility & performance** |\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Simple Analogy:\n",
        "> Imagine you're trying to guess the number of candies in a jar.  \n",
        "> One person might be off ‚Äî but if 50 people guess and you take the average, you're probably close.  \n",
        "That‚Äôs **ensemble learning** ‚Äî combining multiple perspectives (models) to get a better answer.\n",
        "\n",
        "---\n",
        "\n",
        "## üî• Real-World Examples\n",
        "\n",
        "- **Random Forest** = Bagging with decision trees\n",
        "- **XGBoost / LightGBM** = Powerful boosting algorithms\n",
        "- **Voting Classifier** = Simple ensemble using majority voting\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ In Code (Tiny Example with Scikit-learn)\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Combine three different models\n",
        "ensemble = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('lr', LogisticRegression()),\n",
        "        ('dt', DecisionTreeClassifier()),\n",
        "        ('svm', SVC(probability=True))\n",
        "    ],\n",
        "    voting='soft'\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "15 When should we avoid using ensemble methods\n",
        "-Awesome question ‚Äî because while **ensemble methods** are powerful, they‚Äôre **not always the right choice**.\n",
        "\n",
        "Here‚Äôs when you might want to **avoid** using them:\n",
        "\n",
        "---\n",
        "\n",
        "## üö´ **1. When Interpretability is Crucial**\n",
        "\n",
        "Ensemble models like Random Forest or XGBoost are often **black boxes**.  \n",
        "If you're working in fields where you need **clear, human-understandable logic** (like medicine, finance, or law):\n",
        "\n",
        "- Use simpler models like **logistic regression** or **decision trees**.\n",
        "- Ensembles may not provide the transparency needed.\n",
        "\n",
        "---\n",
        "\n",
        "## üêå **2. When Speed (Training or Prediction) is a Priority**\n",
        "\n",
        "Ensembles:\n",
        "- Take **longer to train** (especially boosting methods).\n",
        "- Are **slower at predicting**, since they aggregate outputs from many models.\n",
        "\n",
        "If your task needs **real-time prediction**, you might prefer a **faster, lightweight model**.\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ **3. When You Have Very Little Data**\n",
        "\n",
        "Ensemble models usually perform best with **moderate to large datasets**.  \n",
        "If your dataset is **small**, you risk:\n",
        "\n",
        "- **Overfitting** (especially with boosting)\n",
        "- Poor generalization\n",
        "\n",
        "Sometimes, a **single well-regularized model** will outperform an ensemble on small data.\n",
        "\n",
        "---\n",
        "\n",
        "## üõ†Ô∏è **4. When a Single Model Already Performs Well**\n",
        "\n",
        "If a simple model like a logistic regression or single tree gives you great results:\n",
        "\n",
        "- Adding an ensemble may only give **marginal improvement**.\n",
        "- Not worth the **added complexity and computation**.\n",
        "\n",
        "---\n",
        "\n",
        "## üìâ **5. When the Problem is Already Low-Variance or Low-Bias**\n",
        "\n",
        "Ensemble methods are meant to **correct bias or variance**.  \n",
        "If your model already has:\n",
        "\n",
        "- **Low variance** ‚Üí Bagging won‚Äôt help much.\n",
        "- **Low bias** ‚Üí Boosting might just overfit.\n",
        "\n",
        "---\n",
        "\n",
        "## üí¨ TL;DR ‚Äî Avoid Ensembles When:\n",
        "\n",
        "| Situation                             | Better Option                        |\n",
        "|--------------------------------------|--------------------------------------|\n",
        "| Need **explainable** decisions       | Use interpretable models             |\n",
        "| Need **fast** predictions or training| Use simpler algorithms               |\n",
        "| **Small** dataset                    | Use regularized single models        |\n",
        "| Already **high performance**         | May not need complexity of ensembles |\n",
        "| Risk of **overfitting** is high      | Avoid aggressive boosting            |\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "16 How does Bagging help in reducing overfitting?\n",
        "-Great question ‚Äî and super important in understanding why **bagging** works so well!\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Quick Answer:\n",
        "\n",
        "> **Bagging reduces overfitting by lowering the variance** of high-variance models (like decision trees) through **random sampling** and **model averaging**.\n",
        "\n",
        "---\n",
        "\n",
        "## üìâ The Overfitting Problem:\n",
        "\n",
        "A single **decision tree**, for example, tends to:\n",
        "- Fit the training data **very closely**\n",
        "- Capture **noise** along with signal\n",
        "- Perform poorly on new, unseen data (i.e., **overfit**)\n",
        "\n",
        "---\n",
        "\n",
        "## üå≥ How Bagging Helps:\n",
        "\n",
        "### ‚úÖ 1. **Bootstrap Sampling**\n",
        "- Bagging creates **multiple different versions** of the training dataset using **random sampling with replacement**.\n",
        "- Each model sees **different data**, so they **learn different patterns**.\n",
        "\n",
        "### ‚úÖ 2. **Independent Training**\n",
        "- Each base model (usually a high-variance learner like a tree) is trained **independently** on its own data.\n",
        "- As a result, they make **uncorrelated errors**.\n",
        "\n",
        "### ‚úÖ 3. **Averaging Predictions**\n",
        "- For regression: take the **average** of predictions.\n",
        "- For classification: take a **majority vote**.\n",
        "- This averaging process **smooths out the noise** and cancels out extreme predictions.\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Analogy: Crowd Wisdom\n",
        "\n",
        "Imagine asking 100 people to guess the weight of a watermelon:\n",
        "- One person might be very wrong.\n",
        "- But the **average of all guesses** is likely close to the true value.\n",
        "\n",
        "Bagging does the same: reduces **extreme variance** by blending predictions.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Example: Decision Tree vs Bagged Trees\n",
        "\n",
        "| Feature                | Single Tree       | Bagging (Random Forest) |\n",
        "|------------------------|-------------------|--------------------------|\n",
        "| Accuracy (train)       | üî• Very high       | ‚úÖ High                  |\n",
        "| Accuracy (test)        | üò¨ Low (overfit)   | ‚úÖ Higher                |\n",
        "| Sensitivity to noise   | ‚ùå High            | ‚úÖ Low                   |\n",
        "\n",
        "---\n",
        "\n",
        "## üí° TL;DR\n",
        "\n",
        "> **Bagging = Diversify + Average**  \n",
        "> It makes high-variance models more stable by averaging out their noise ‚Äî and that‚Äôs how it helps reduce **overfitting**.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "17  Why is Random Forest better than a single Decision Tree?\n",
        "-Great question ‚Äî and one of the most common ones in machine learning! Let's break it down clearly:\n",
        "\n",
        "---\n",
        "\n",
        "## üå≤ Why is **Random Forest** Better Than a Single **Decision Tree**?\n",
        "\n",
        "### ‚úÖ **Short Answer:**\n",
        "> A **Random Forest** is usually better because it reduces **overfitting**, increases **accuracy**, and improves **generalization** by combining the predictions of **many diverse decision trees**.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Deep Dive: Key Differences\n",
        "\n",
        "| Feature | üß± **Single Decision Tree** | üå≥ **Random Forest** |\n",
        "|--------|----------------------------|----------------------|\n",
        "| **Accuracy** | May overfit the training data | More accurate and stable |\n",
        "| **Overfitting** | High (especially deep trees) | Much lower |\n",
        "| **Bias** | Low | Slightly higher |\n",
        "| **Variance** | High | Lower (because of averaging) |\n",
        "| **Robustness** | Sensitive to small data changes | Robust and consistent |\n",
        "| **Interpretability** | Easy to interpret | Harder to interpret |\n",
        "| **Performance on unseen data** | Often poor | Typically much better |\n",
        "\n",
        "---\n",
        "\n",
        "## üîß How Random Forest Improves Performance:\n",
        "\n",
        "### 1. **Bagging (Bootstrap Aggregating)**\n",
        "- Trains each tree on a **random subset** of the data.\n",
        "- Reduces **variance** and **overfitting**.\n",
        "\n",
        "### 2. **Feature Randomness**\n",
        "- Each tree considers only a **random subset of features** at each split.\n",
        "- Increases **diversity** between trees ‚Üí better overall predictions.\n",
        "\n",
        "### 3. **Aggregation**\n",
        "- Final prediction is made via **majority vote** (classification) or **averaging** (regression).\n",
        "- Helps **smooth out errors** of individual trees.\n",
        "\n",
        "---\n",
        "\n",
        "## üìâ Visual Analogy:\n",
        "\n",
        "Imagine a single decision tree as one very opinionated person.  \n",
        "A random forest is like a **committee** of diverse people ‚Äî each gives their opinion, and the final decision is a **group vote**, which is more balanced.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Summary:\n",
        "\n",
        "> Random Forest = **Many weak models** (trees) + **Smart combination**  \n",
        "‚Üí Results in a **strong**, **robust**, and **accurate** predictor.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "18 What is the role of bootstrap sampling in Bagging?\n",
        "-Great one! Bootstrap sampling is **at the heart of Bagging**, so let‚Äôs break it down simply:\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ **Role of Bootstrap Sampling in Bagging**\n",
        "\n",
        "> **Bootstrap sampling** allows Bagging to train **multiple diverse models** on slightly **different datasets**, even though they all come from the **same original data**.\n",
        "\n",
        "This helps Bagging reduce **variance** and avoid **overfitting**.\n",
        "\n",
        "---\n",
        "\n",
        "## üß∫ What is Bootstrap Sampling?\n",
        "\n",
        "- It's a technique where we **randomly sample** from the dataset **with replacement**.\n",
        "- Each sample is the **same size as the original dataset**, but some points may appear multiple times, and others may be left out.\n",
        "\n",
        "üìå So each model gets a **slightly different version** of the training data.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Why Is This Important?\n",
        "\n",
        "### ‚úÖ 1. **Introduces Diversity**\n",
        "- Each base model (like a decision tree) learns from **different data**, so they make **different errors**.\n",
        "- Diversity among models is key to ensemble success.\n",
        "\n",
        "### ‚úÖ 2. **Reduces Variance**\n",
        "- Individual trees are high-variance learners (they change a lot with small data changes).\n",
        "- By training them on different datasets and **averaging their outputs**, we smooth out their predictions.\n",
        "\n",
        "### ‚úÖ 3. **Supports OOB (Out-of-Bag) Evaluation**\n",
        "- Since some data points are left out in each bootstrap sample, they can be used to **evaluate model performance** without needing a separate validation set.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Real-Life Analogy:\n",
        "\n",
        "Imagine you ask 5 different friends to solve a puzzle, but each one is given a **slightly shuffled version** of the clues.\n",
        "\n",
        "- Individually, their answers might vary.\n",
        "- But if you take the **average or majority vote**, you're likely to get the correct solution.\n",
        "- That's what bootstrap sampling enables.\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Summary\n",
        "\n",
        "> üîÅ **Bootstrap Sampling** in Bagging helps:\n",
        "- Create **diverse models**\n",
        "- Reduce **overfitting**\n",
        "- Increase **generalization**\n",
        "- Enable **out-of-bag evaluation**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "19 What are some real-world applications of ensemble techniques?\n",
        "-Awesome question ‚Äî ensemble techniques are everywhere in the real world because they **boost accuracy, reliability, and robustness**. Let‚Äôs check out some top applications across different industries:\n",
        "\n",
        "---\n",
        "\n",
        "## üåç Real-World Applications of Ensemble Techniques\n",
        "\n",
        "---\n",
        "\n",
        "### üè¶ **1. Finance**\n",
        "- **Credit Scoring** ‚Äì Predict loan defaults using models like Random Forest or XGBoost.\n",
        "- **Fraud Detection** ‚Äì Ensemble models catch subtle patterns in transaction data.\n",
        "- **Stock Price Prediction** ‚Äì Boosting models help forecast trends by combining signals.\n",
        "\n",
        "---\n",
        "\n",
        "### üè• **2. Healthcare**\n",
        "- **Disease Diagnosis** ‚Äì Ensemble models like stacking can outperform doctors in some diagnostic tasks.\n",
        "- **Medical Image Classification** ‚Äì Detect tumors or anomalies in X-rays or MRIs using ensembles of CNNs.\n",
        "- **Risk Prediction** ‚Äì Predict patient outcomes like diabetes or heart failure using Random Forests or Gradient Boosting.\n",
        "\n",
        "---\n",
        "\n",
        "### üõçÔ∏è **3. E-commerce & Retail**\n",
        "- **Recommendation Systems** ‚Äì Stacking different algorithms to give better product recommendations.\n",
        "- **Customer Churn Prediction** ‚Äì Use ensemble models to identify users likely to leave.\n",
        "- **Price Optimization** ‚Äì Forecast demand and dynamically adjust prices with boosted regressors.\n",
        "\n",
        "---\n",
        "\n",
        "### ü§ñ **4. Autonomous Vehicles**\n",
        "- **Object Detection** ‚Äì Ensemble methods combine multiple vision models to improve detection accuracy.\n",
        "- **Path Planning** ‚Äì Use voting-based ensembles to ensure safe navigation choices.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† **5. Natural Language Processing (NLP)**\n",
        "- **Sentiment Analysis** ‚Äì Combine different models (like RNNs + transformers) for better sentiment detection.\n",
        "- **Spam Filtering** ‚Äì Bagging and boosting help detect spam emails more accurately.\n",
        "\n",
        "---\n",
        "\n",
        "### üéÆ **6. Gaming & AI**\n",
        "- **Game Bot Decision Making** ‚Äì Use ensembles to create smarter, adaptive agents.\n",
        "- **Cheat Detection** ‚Äì Spot suspicious behavior using ensemble classifiers.\n",
        "\n",
        "---\n",
        "\n",
        "### üö® **7. Cybersecurity**\n",
        "- **Intrusion Detection** ‚Äì Ensemble models monitor network traffic and detect anomalies.\n",
        "- **Malware Classification** ‚Äì Boosted trees can classify malware based on behavioral signatures.\n",
        "\n",
        "---\n",
        "\n",
        "## üîß Summary\n",
        "\n",
        "| Industry      | Use Case Example                          | Ensemble Type Used        |\n",
        "|---------------|--------------------------------------------|---------------------------|\n",
        "| Finance       | Fraud detection                            | Random Forest, XGBoost    |\n",
        "| Healthcare    | Disease prediction                         | Stacking, Gradient Boosting|\n",
        "| Retail        | Product recommendations                    | Voting, Stacking          |\n",
        "| Autonomous Cars| Object detection, planning                | Bagging + Deep Ensembles  |\n",
        "| NLP           | Sentiment/spam filtering                   | Stacking, Boosting        |\n",
        "| Cybersecurity | Anomaly detection                          | Bagging, Isolation Forest |\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "20 What is the difference between Bagging and Boosting?\n",
        "-üÜö Bagging vs Boosting\n",
        "Feature\tüß∫ Bagging\tüöÄ Boosting\n",
        "Full Name\tBootstrap Aggregating\t‚Äî\n",
        "Main Goal\tReduce variance\tReduce bias and variance\n",
        "Model Training\tModels trained independently in parallel\tModels trained sequentially, one after another\n",
        "Data Sampling\tUses bootstrap samples (with replacement)\tEach model focuses on mistakes of the previous one\n",
        "Weighting\tAll models are equal in voting/averaging\tLater models get more weight (adaptive)\n",
        "Overfitting\tHelps prevent overfitting\tCan overfit if not properly regularized\n",
        "Best For\tHigh-variance models (e.g., decision trees)\tWeak learners needing bias correction\n",
        "Popular Algorithms\tRandom Forest\tAdaBoost, Gradient Boosting, XGBoost\n",
        "\n"
      ],
      "metadata": {
        "id": "XuXzj2fmZPDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "**Practical**\n",
        "\n",
        "\n",
        "\n",
        "21 Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy\n",
        "-import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with Decision Trees as base estimators\n",
        "# n_estimators specifies the number of trees in the ensemble.\n",
        "# random_state for reproducibility.\n",
        "bagging_clf = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the Bagging Classifier\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Example of how to change the base estimator parameters.\n",
        "# For example, to limit the maximum depth of the decision trees:\n",
        "\n",
        "bagging_clf_depth_limited = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(max_depth=5),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bagging_clf_depth_limited.fit(X_train, y_train)\n",
        "y_pred_depth_limited = bagging_clf_depth_limited.predict(X_test)\n",
        "accuracy_depth_limited = accuracy_score(y_test, y_pred_depth_limited)\n",
        "\n",
        "print(f\"Bagging Classifier Accuracy (max_depth=5): {accuracy_depth_limited:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "22 Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)\n",
        "-import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Generate a sample regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Bagging Regressor with Decision Trees as base estimators\n",
        "bagging_reg = BaggingRegressor(\n",
        "    base_estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the Bagging Regressor\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = bagging_reg.predict(X_test)\n",
        "\n",
        "# Calculate the Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Print the MSE\n",
        "print(f\"Bagging Regressor Mean Squared Error (MSE): {mse:.4f}\")\n",
        "\n",
        "#Example of changing base estimator parameters, such as max_depth.\n",
        "bagging_reg_depth_limited = BaggingRegressor(\n",
        "    base_estimator=DecisionTreeRegressor(max_depth=5),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bagging_reg_depth_limited.fit(X_train, y_train)\n",
        "y_pred_depth_limited = bagging_reg_depth_limited.predict(X_test)\n",
        "mse_depth_limited = mean_squared_error(y_test, y_pred_depth_limited)\n",
        "\n",
        "print(f\"Bagging Regressor Mean Squared Error (MSE) (max_depth=5): {mse_depth_limited:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "23 Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores\n",
        "-import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "feature_names = cancer.feature_names\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the Random Forest Classifier\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance scores\n",
        "feature_importance = rf_classifier.feature_importances_\n",
        "\n",
        "# Create a DataFrame to display feature importance\n",
        "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
        "\n",
        "# Sort the DataFrame by importance in descending order\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print the feature importance scores\n",
        "print(\"Feature Importance Scores:\")\n",
        "print(feature_importance_df)\n",
        "\n",
        "#Example of accessing a specific features importance\n",
        "print(\"\\nImportance of 'mean radius':\")\n",
        "print(feature_importance_df[feature_importance_df['Feature'] == 'mean radius']['Importance'].values[0])\n",
        "\n",
        "\n",
        "24 Train a Random Forest Regressor and compare its performance with a single Decision Tree\n",
        "-import numpy as np\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate a sample regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Regressor\n",
        "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Train the Random Forest Regressor\n",
        "rf_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Train the Decision Tree Regressor\n",
        "dt_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "rf_y_pred = rf_regressor.predict(X_test)\n",
        "dt_y_pred = dt_regressor.predict(X_test)\n",
        "\n",
        "# Calculate the Mean Squared Error (MSE) for both models\n",
        "rf_mse = mean_squared_error(y_test, rf_y_pred)\n",
        "dt_mse = mean_squared_error(y_test, dt_y_pred)\n",
        "\n",
        "# Print the MSE for both models\n",
        "print(f\"Random Forest Regressor MSE: {rf_mse:.4f}\")\n",
        "print(f\"Decision Tree Regressor MSE: {dt_mse:.4f}\")\n",
        "\n",
        "# Compare the performance\n",
        "if rf_mse < dt_mse:\n",
        "    print(\"\\nRandom Forest Regressor performs better than Decision Tree Regressor.\")\n",
        "else:\n",
        "    print(\"\\nDecision Tree Regressor performs better than Random Forest Regressor (or they perform equally).\")\n",
        "\n",
        "# Example of how to change the Random Forest parameters:\n",
        "rf_regressor_modified = RandomForestRegressor(n_estimators = 50, max_depth = 5, random_state = 42)\n",
        "rf_regressor_modified.fit(X_train, y_train)\n",
        "rf_y_pred_modified = rf_regressor_modified.predict(X_test)\n",
        "rf_mse_modified = mean_squared_error(y_test, rf_y_pred_modified)\n",
        "print(f\"Random Forest Regressor MSE (n_estimators=50, max_depth=5): {rf_mse_modified:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "25 Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier\n",
        "-import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Generate a sample classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier with oob_score=True\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
        "\n",
        "# Train the Random Forest Classifier\n",
        "rf_classifier.fit(X, y)\n",
        "\n",
        "# Get the Out-of-Bag (OOB) score\n",
        "oob_score = rf_classifier.oob_score_\n",
        "\n",
        "# Print the OOB score\n",
        "print(f\"Out-of-Bag (OOB) Score: {oob_score:.4f}\")\n",
        "\n",
        "#Example of accessing the oob_decision_function_\n",
        "oob_decision_function = rf_classifier.oob_decision_function_\n",
        "\n",
        "print(\"\\nOOB Decision Function shape:\", oob_decision_function.shape)\n",
        "print(\"Example oob_decision_function for the first 5 samples:\\n\", oob_decision_function[:5])\n",
        "\n",
        "\n",
        "\n",
        "26 Train a Bagging Classifier using SVM as a base estimator and print accuracy\n",
        "-import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate a sample classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with SVM as the base estimator\n",
        "bagging_svm = BaggingClassifier(\n",
        "    base_estimator=SVC(probability=True),  # probability=True needed for predict_proba\n",
        "    n_estimators=10,  # Reduced estimators due to SVM's computational cost\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the Bagging Classifier\n",
        "bagging_svm.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = bagging_svm.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Bagging SVM Classifier Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "#Example of changing base estimator parameters.\n",
        "bagging_svm_modified = BaggingClassifier(\n",
        "    base_estimator=SVC(probability=True, C=0.5, kernel='linear'),\n",
        "    n_estimators=10,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bagging_svm_modified.fit(X_train, y_train)\n",
        "y_pred_modified = bagging_svm_modified.predict(X_test)\n",
        "accuracy_modified = accuracy_score(y_test, y_pred_modified)\n",
        "\n",
        "print(f\"Bagging SVM Classifier Accuracy (C=0.5, kernel='linear'): {accuracy_modified:.4f}\")\n",
        "\n",
        "\n",
        "27 Train a Random Forest Classifier with different numbers of trees and compare accuracy\n",
        "-import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the range of number of trees to test\n",
        "n_estimators_range = [10, 50, 100, 200, 300, 400, 500]\n",
        "\n",
        "# Store the accuracies for each number of trees\n",
        "accuracies = []\n",
        "\n",
        "# Train and evaluate the Random Forest Classifier for each number of trees\n",
        "for n_estimators in n_estimators_range:\n",
        "    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n",
        "    rf_classifier.fit(X_train, y_train)\n",
        "    y_pred = rf_classifier.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(accuracy)\n",
        "    print(f\"Number of Trees: {n_estimators}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Plot the accuracy vs. number of trees\n",
        "plt.plot(n_estimators_range, accuracies, marker='o')\n",
        "plt.title(\"Random Forest Accuracy vs. Number of Trees\")\n",
        "plt.xlabel(\"Number of Trees (n_estimators)\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Find the best number of trees and corresponding accuracy\n",
        "best_n_estimators = n_estimators_range[np.argmax(accuracies)]\n",
        "best_accuracy = max(accuracies)\n",
        "\n",
        "print(f\"\\nBest Number of Trees: {best_n_estimators}, Best Accuracy: {best_accuracy:.4f}\")\n",
        "\n",
        "#Example of changing other Random Forest parameters.\n",
        "rf_classifier_modified = RandomForestClassifier(n_estimators = 100, max_depth = 5, random_state = 42)\n",
        "rf_classifier_modified.fit(X_train, y_train)\n",
        "y_pred_modified = rf_classifier_modified.predict(X_test)\n",
        "accuracy_modified = accuracy_score(y_test, y_pred_modified)\n",
        "\n",
        "print(f\"Random Forest Classifier Accuracy (n_estimators=100, max_depth=5): {accuracy_modified:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "28 Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score\n",
        "-import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Generate a sample classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with Logistic Regression as the base estimator\n",
        "bagging_lr = BaggingClassifier(\n",
        "    base_estimator=LogisticRegression(solver='liblinear'), # solver is important for small datasets\n",
        "    n_estimators=10,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the Bagging Classifier\n",
        "bagging_lr.fit(X_train, y_train)\n",
        "\n",
        "# Make probability predictions on the test set\n",
        "y_prob = bagging_lr.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\n",
        "\n",
        "# Calculate the AUC score\n",
        "auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "# Print the AUC score\n",
        "print(f\"Bagging Logistic Regression AUC: {auc:.4f}\")\n",
        "\n",
        "# Example of changing Logistic Regression parameters.\n",
        "bagging_lr_modified = BaggingClassifier(\n",
        "    base_estimator=LogisticRegression(solver='liblinear', C=0.5),\n",
        "    n_estimators=10,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bagging_lr_modified.fit(X_train, y_train)\n",
        "y_prob_modified = bagging_lr_modified.predict_proba(X_test)[:, 1]\n",
        "auc_modified = roc_auc_score(y_test, y_prob_modified)\n",
        "\n",
        "print(f\"Bagging Logistic Regression AUC (C=0.5): {auc_modified:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "29 Train a Random Forest Regressor and analyze feature importance scores\n",
        "-import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate a sample regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the Random Forest Regressor\n",
        "rf_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance scores\n",
        "feature_importance = rf_regressor.feature_importances_\n",
        "\n",
        "# Create a DataFrame to display feature importance\n",
        "feature_importance_df = pd.DataFrame({'Feature': range(X.shape[1]), 'Importance': feature_importance})\n",
        "\n",
        "# Sort the DataFrame by importance in descending order\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print the feature importance scores\n",
        "print(\"Feature Importance Scores:\")\n",
        "print(feature_importance_df)\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
        "plt.xlabel(\"Feature Index\")\n",
        "plt.ylabel(\"Importance Score\")\n",
        "plt.title(\"Random Forest Regressor Feature Importance\")\n",
        "plt.xticks(feature_importance_df['Feature']) #set ticks to feature numbers\n",
        "plt.show()\n",
        "\n",
        "# Example of accessing a specific features importance\n",
        "print(f\"\\nImportance score of feature index 0: {feature_importance_df[feature_importance_df['Feature'] == 0]['Importance'].values[0]}\")\n",
        "\n",
        "#Example of changing RF parameters and analyzing feature importance.\n",
        "rf_regressor_modified = RandomForestRegressor(n_estimators=50, max_depth=5, random_state=42)\n",
        "rf_regressor_modified.fit(X_train, y_train)\n",
        "feature_importance_modified = rf_regressor_modified.feature_importances_\n",
        "feature_importance_df_modified = pd.DataFrame({'Feature': range(X.shape[1]), 'Importance': feature_importance_modified})\n",
        "feature_importance_df_modified = feature_importance_df_modified.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importance Scores (Modified RF):\")\n",
        "print(feature_importance_df_modified)\n",
        "\n",
        "\n",
        "\n",
        "30 Train an ensemble model using both Bagging and Random Forest and compare accuracy\n",
        "-import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, VotingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a sample classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create individual models\n",
        "bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train individual models\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions with individual models\n",
        "bagging_y_pred = bagging_clf.predict(X_test)\n",
        "rf_y_pred = rf_clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy for individual models\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_y_pred)\n",
        "rf_accuracy = accuracy_score(y_test, rf_y_pred)\n",
        "\n",
        "print(f\"Bagging Classifier Accuracy: {bagging_accuracy:.4f}\")\n",
        "print(f\"Random Forest Classifier Accuracy: {rf_accuracy:.4f}\")\n",
        "\n",
        "# Create a Voting Classifier (ensemble of Bagging and Random Forest)\n",
        "ensemble_clf = VotingClassifier(\n",
        "    estimators=[('bagging', bagging_clf), ('random_forest', rf_clf)],\n",
        "    voting='hard'  # 'hard' voting uses predicted class labels\n",
        ")\n",
        "\n",
        "# Train the ensemble model\n",
        "ensemble_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions with the ensemble model\n",
        "ensemble_y_pred = ensemble_clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy for the ensemble model\n",
        "ensemble_accuracy = accuracy_score(y_test, ensemble_y_pred)\n",
        "\n",
        "print(f\"Ensemble Classifier Accuracy: {ensemble_accuracy:.4f}\")\n",
        "\n",
        "#Compare accuracies\n",
        "if ensemble_accuracy > max(bagging_accuracy, rf_accuracy):\n",
        "  print(\"\\nThe Ensemble model performed better than the individual models.\")\n",
        "else:\n",
        "  print(\"\\nThe Ensemble model did not outperform the best individual model.\")\n",
        "\n",
        "#Example of changing the voting method to soft, which requires predict_proba.\n",
        "ensemble_clf_soft = VotingClassifier(\n",
        "    estimators=[('bagging', bagging_clf), ('random_forest', rf_clf)],\n",
        "    voting='soft' #soft voting uses predicted probabilities\n",
        ")\n",
        "ensemble_clf_soft.fit(X_train, y_train)\n",
        "ensemble_y_pred_soft = ensemble_clf_soft.predict(X_test)\n",
        "ensemble_accuracy_soft = accuracy_score(y_test, ensemble_y_pred_soft)\n",
        "print(f\"Ensemble Classifier Accuracy (soft voting): {ensemble_accuracy_soft:.4f}\")\n",
        "\n",
        "\n",
        "31 Train a Random Forest Classifier and tune hyperparameters using GridSearchCV\n",
        "-import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define the hyperparameters to tune\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=3, scoring='accuracy', verbose=2, n_jobs=-1)\n",
        "\n",
        "# Perform the grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "\n",
        "# Get the best model\n",
        "best_rf_classifier = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_rf_classifier.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the best model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Best Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "#Example of accessing the grid search results.\n",
        "print(\"\\nGrid Search Results:\")\n",
        "print(grid_search.cv_results_)\n",
        "\n",
        "\n",
        "\n",
        "32 Train a Bagging Regressor with different numbers of base estimators and compare performance\n",
        "-import numpy as np\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate a sample regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the range of number of estimators to test\n",
        "n_estimators_range = [10, 50, 100, 200, 300, 400, 500]\n",
        "\n",
        "# Store the MSE for each number of estimators\n",
        "mses = []\n",
        "\n",
        "# Train and evaluate the Bagging Regressor for each number of estimators\n",
        "for n_estimators in n_estimators_range:\n",
        "    bagging_regressor = BaggingRegressor(\n",
        "        base_estimator=DecisionTreeRegressor(),\n",
        "        n_estimators=n_estimators,\n",
        "        random_state=42\n",
        "    )\n",
        "    bagging_regressor.fit(X_train, y_train)\n",
        "    y_pred = bagging_regressor.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mses.append(mse)\n",
        "    print(f\"Number of Estimators: {n_estimators}, MSE: {mse:.4f}\")\n",
        "\n",
        "# Plot the MSE vs. number of estimators\n",
        "plt.plot(n_estimators_range, mses, marker='o')\n",
        "plt.title(\"Bagging Regressor MSE vs. Number of Estimators\")\n",
        "plt.xlabel(\"Number of Estimators (n_estimators)\")\n",
        "plt.ylabel(\"Mean Squared Error (MSE)\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Find the best number of estimators (lowest MSE)\n",
        "best_n_estimators = n_estimators_range[np.argmin(mses)]\n",
        "best_mse = min(mses)\n",
        "\n",
        "print(f\"\\nBest Number of Estimators: {best_n_estimators}, Best MSE: {best_mse:.4f}\")\n",
        "\n",
        "#Example of changing base estimator parameters.\n",
        "bagging_regressor_modified = BaggingRegressor(\n",
        "    base_estimator=DecisionTreeRegressor(max_depth=5),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_regressor_modified.fit(X_train, y_train)\n",
        "y_pred_modified = bagging_regressor_modified.predict(X_test)\n",
        "mse_modified = mean_squared_error(y_test, y_pred_modified)\n",
        "print(f\"Bagging Regressor MSE (max_depth=5): {mse_modified:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "33 Train a Random Forest Classifier and analyze misclassified samples\n",
        "-import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "feature_names = cancer.feature_names\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the Random Forest Classifier\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# Analyze misclassified samples\n",
        "misclassified_indices = np.where(y_pred != y_test)[0]\n",
        "misclassified_samples = X_test[misclassified_indices]\n",
        "misclassified_true_labels = y_test[misclassified_indices]\n",
        "misclassified_predicted_labels = y_pred[misclassified_indices]\n",
        "\n",
        "# Print misclassified samples and their true/predicted labels\n",
        "print(\"Misclassified Samples:\")\n",
        "for i in range(len(misclassified_indices)):\n",
        "    print(f\"Sample Index: {misclassified_indices[i]}\")\n",
        "    print(f\"True Label: {misclassified_true_labels[i]}, Predicted Label: {misclassified_predicted_labels[i]}\")\n",
        "    print(f\"Features: {misclassified_samples[i]}\")\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "# Create a confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=cancer.target_names, yticklabels=cancer.target_names)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Example of accessing the features of a single misclassified sample.\n",
        "if len(misclassified_indices) > 0:\n",
        "    first_misclassified_sample_features = misclassified_samples[0]\n",
        "    print(f\"\\nFeatures of the first misclassified sample:\\n{first_misclassified_sample_features}\")\n",
        "else:\n",
        "    print(\"\\nNo misclassified samples.\")\n",
        "\n",
        "\n",
        "\n",
        "34 Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier\n",
        "-import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a sample classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a single Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with Decision Trees as base estimators\n",
        "bagging_classifier = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(random_state=42),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the Decision Tree Classifier\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Train the Bagging Classifier\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "dt_y_pred = dt_classifier.predict(X_test)\n",
        "bagging_y_pred = bagging_classifier.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of both models\n",
        "dt_accuracy = accuracy_score(y_test, dt_y_pred)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_y_pred)\n",
        "\n",
        "# Print the accuracies\n",
        "print(f\"Decision Tree Classifier Accuracy: {dt_accuracy:.4f}\")\n",
        "print(f\"Bagging Classifier Accuracy: {bagging_accuracy:.4f}\")\n",
        "\n",
        "# Compare the performance\n",
        "if bagging_accuracy > dt_accuracy:\n",
        "    print(\"\\nBagging Classifier performs better than Decision Tree Classifier.\")\n",
        "elif bagging_accuracy < dt_accuracy:\n",
        "    print(\"\\nDecision Tree Classifier performs better than Bagging Classifier.\")\n",
        "else:\n",
        "    print(\"\\nBoth classifiers have the same accuracy.\")\n",
        "\n",
        "#Example of changing the base estimator parameters.\n",
        "bagging_classifier_modified = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(max_depth=5, random_state=42),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_classifier_modified.fit(X_train, y_train)\n",
        "bagging_y_pred_modified = bagging_classifier_modified.predict(X_test)\n",
        "bagging_accuracy_modified = accuracy_score(y_test, bagging_y_pred_modified)\n",
        "print(f\"Bagging Classifier Accuracy (max_depth=5): {bagging_accuracy_modified:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "35 Train a Random Forest Classifier and visualize the confusion matrix\n",
        "-import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "class_names = cancer.target_names\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create and train a Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix for Random Forest Classifier')\n",
        "plt.show()\n",
        "\n",
        "# Example of accessing the confusion matrix values.\n",
        "print(\"\\nConfusion Matrix Values:\")\n",
        "print(f\"True Negatives (TN): {cm[0, 0]}\")\n",
        "print(f\"False Positives (FP): {cm[0, 1]}\")\n",
        "print(f\"False Negatives (FN): {cm[1, 0]}\")\n",
        "print(f\"True Positives (TP): {cm[1, 1]}\")\n",
        "\n",
        "\n",
        "\n",
        "36 Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy\n",
        "-import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a sample classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the base estimators\n",
        "estimators = [\n",
        "    ('dt', DecisionTreeClassifier(random_state=42)),\n",
        "    ('svm', SVC(probability=True, random_state=42)),  # probability=True for predict_proba\n",
        "    ('lr', LogisticRegression(solver='liblinear', random_state=42))\n",
        "]\n",
        "\n",
        "# Define the final estimator (meta-learner)\n",
        "final_estimator = LogisticRegression(solver='liblinear', random_state=42)\n",
        "\n",
        "# Create the Stacking Classifier\n",
        "stacking_clf = StackingClassifier(estimators=estimators, final_estimator=final_estimator)\n",
        "\n",
        "# Train the Stacking Classifier\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "stacking_y_pred = stacking_clf.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the Stacking Classifier\n",
        "stacking_accuracy = accuracy_score(y_test, stacking_y_pred)\n",
        "\n",
        "print(f\"Stacking Classifier Accuracy: {stacking_accuracy:.4f}\")\n",
        "\n",
        "# Train and evaluate individual models for comparison\n",
        "dt_clf = DecisionTreeClassifier(random_state=42)\n",
        "svm_clf = SVC(random_state=42)\n",
        "lr_clf = LogisticRegression(solver='liblinear', random_state=42)\n",
        "\n",
        "dt_clf.fit(X_train, y_train)\n",
        "svm_clf.fit(X_train, y_train)\n",
        "lr_clf.fit(X_train, y_train)\n",
        "\n",
        "dt_y_pred = dt_clf.predict(X_test)\n",
        "svm_y_pred = svm_clf.predict(X_test)\n",
        "lr_y_pred = lr_clf.predict(X_test)\n",
        "\n",
        "dt_accuracy = accuracy_score(y_test, dt_y_pred)\n",
        "svm_accuracy = accuracy_score(y_test, svm_y_pred)\n",
        "lr_accuracy = accuracy_score(y_test, lr_y_pred)\n",
        "\n",
        "print(f\"Decision Tree Accuracy: {dt_accuracy:.4f}\")\n",
        "print(f\"SVM Accuracy: {svm_accuracy:.4f}\")\n",
        "print(f\"Logistic Regression Accuracy: {lr_accuracy:.4f}\")\n",
        "\n",
        "# Compare the performance\n",
        "if stacking_accuracy > max(dt_accuracy, svm_accuracy, lr_accuracy):\n",
        "    print(\"\\nStacking Classifier performs better than individual models.\")\n",
        "else:\n",
        "    print(\"\\nStacking Classifier did not outperform the best individual model.\")\n",
        "\n",
        "#Example of changing final estimator.\n",
        "final_estimator_modified = DecisionTreeClassifier(random_state = 42)\n",
        "stacking_clf_modified = StackingClassifier(estimators=estimators, final_estimator=final_estimator_modified)\n",
        "stacking_clf_modified.fit(X_train, y_train)\n",
        "stacking_y_pred_modified = stacking_clf_modified.predict(X_test)\n",
        "stacking_accuracy_modified = accuracy_score(y_test, stacking_y_pred_modified)\n",
        "print(f\"Stacking Classifier Accuracy (final estimator = Decision Tree): {stacking_accuracy_modified:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "37  Train a Random Forest Classifier and print the top 5 most important features\n",
        "-import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "feature_names = cancer.feature_names\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the Random Forest Classifier\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance scores\n",
        "feature_importance = rf_classifier.feature_importances_\n",
        "\n",
        "# Create a DataFrame to display feature importance\n",
        "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
        "\n",
        "# Sort the DataFrame by importance in descending order\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print the top 5 most important features\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(feature_importance_df.head(5))\n",
        "\n",
        "\n",
        "38 Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score\n",
        "-import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Generate a sample classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with Decision Trees as base estimators\n",
        "bagging_clf = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the Bagging Classifier\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "# Calculate Precision, Recall, and F1-score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n",
        "\n",
        "#Example of changing averaging method for multiclass problems.\n",
        "#Consider a multiclass problem:\n",
        "\n",
        "X_multi, y_multi = make_classification(n_samples=1000, n_features=20, n_classes=3, random_state=42)\n",
        "X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(X_multi, y_multi, test_size=0.3, random_state=42)\n",
        "bagging_clf_multi = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
        "bagging_clf_multi.fit(X_train_multi, y_train_multi)\n",
        "y_pred_multi = bagging_clf_multi.predict(X_test_multi)\n",
        "precision_macro = precision_score(y_test_multi, y_pred_multi, average='macro')\n",
        "recall_macro = recall_score(y_test_multi, y_pred_multi, average='macro')\n",
        "f1_macro = f1_score(y_test_multi, y_pred_multi, average='macro')\n",
        "\n",
        "print(f\"\\nMulticlass Precision (macro): {precision_macro:.4f}\")\n",
        "print(f\"Multiclass Recall (macro): {recall_macro:.4f}\")\n",
        "print(f\"Multiclass F1-score (macro): {f1_macro:.4f}\")\n",
        "\n",
        "\n",
        "39 Train a Random Forest Classifier and analyze the effect of max_depth on accuracy\n",
        "-import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the range of max_depth values to test\n",
        "max_depth_range = [None, 5, 10, 15, 20, 30, 40, 50]\n",
        "\n",
        "# Store the accuracies for each max_depth\n",
        "accuracies = []\n",
        "\n",
        "# Train and evaluate the Random Forest Classifier for each max_depth\n",
        "for max_depth in max_depth_range:\n",
        "    rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=max_depth, random_state=42)\n",
        "    rf_classifier.fit(X_train, y_train)\n",
        "    y_pred = rf_classifier.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(accuracy)\n",
        "    print(f\"Max Depth: {max_depth}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Plot the accuracy vs. max_depth\n",
        "plt.plot(max_depth_range, accuracies, marker='o')\n",
        "plt.title(\"Random Forest Accuracy vs. Max Depth\")\n",
        "plt.xlabel(\"Max Depth\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Find the best max_depth and corresponding accuracy\n",
        "best_max_depth = max_depth_range[np.argmax(accuracies)]\n",
        "best_accuracy = max(accuracies)\n",
        "\n",
        "print(f\"\\nBest Max Depth: {best_max_depth}, Best Accuracy: {best_accuracy:.4f}\")\n",
        "\n",
        "#Example of changing n_estimators while keeping max_depth constant.\n",
        "rf_classifier_modified = RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42)\n",
        "rf_classifier_modified.fit(X_train, y_train)\n",
        "y_pred_modified = rf_classifier_modified.predict(X_test)\n",
        "accuracy_modified = accuracy_score(y_test, y_pred_modified)\n",
        "print(f\"Random Forest Classifier Accuracy (n_estimators=50, max_depth=10): {accuracy_modified:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "40 Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare\n",
        "performance=\n",
        "-import numpy as np\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate a sample regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create Bagging Regressor with Decision Tree as base estimator\n",
        "bagging_dt = BaggingRegressor(\n",
        "    base_estimator=DecisionTreeRegressor(random_state=42),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Create Bagging Regressor with KNeighbors Regressor as base estimator\n",
        "bagging_knn = BaggingRegressor(\n",
        "    base_estimator=KNeighborsRegressor(),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the Bagging Regressors\n",
        "bagging_dt.fit(X_train, y_train)\n",
        "bagging_knn.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "dt_y_pred = bagging_dt.predict(X_test)\n",
        "knn_y_pred = bagging_knn.predict(X_test)\n",
        "\n",
        "# Calculate the Mean Squared Error (MSE) for both models\n",
        "dt_mse = mean_squared_error(y_test, dt_y_pred)\n",
        "knn_mse = mean_squared_error(y_test, knn_y_pred)\n",
        "\n",
        "# Print the MSE for both models\n",
        "print(f\"Bagging Regressor (Decision Tree) MSE: {dt_mse:.4f}\")\n",
        "print(f\"Bagging Regressor (KNeighbors) MSE: {knn_mse:.4f}\")\n",
        "\n",
        "# Compare the performance\n",
        "if dt_mse < knn_mse:\n",
        "    print(\"\\nBagging Regressor (Decision Tree) performs better.\")\n",
        "elif dt_mse > knn_mse:\n",
        "    print(\"\\nBagging Regressor (KNeighbors) performs better.\")\n",
        "else:\n",
        "    print(\"\\nBoth models have the same performance.\")\n",
        "\n",
        "#Example of changing the KNeighbors Regressor parameters within the Bagging regressor.\n",
        "bagging_knn_modified = BaggingRegressor(\n",
        "    base_estimator = KNeighborsRegressor(n_neighbors = 5, weights = 'distance'),\n",
        "    n_estimators = 100,\n",
        "    random_state = 42\n",
        ")\n",
        "\n",
        "bagging_knn_modified.fit(X_train, y_train)\n",
        "knn_y_pred_modified = bagging_knn_modified.predict(X_test)\n",
        "knn_mse_modified = mean_squared_error(y_test, knn_y_pred_modified)\n",
        "print(f\"Bagging Regressor (KNeighbors, n_neighbors=5, weights='distance') MSE: {knn_mse_modified:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "41 Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score\n",
        "-import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the Random Forest Classifier\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make probability predictions on the test set\n",
        "y_prob = rf_classifier.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\n",
        "\n",
        "# Calculate the ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "# Print the ROC-AUC score\n",
        "print(f\"Random Forest Classifier ROC-AUC: {roc_auc:.4f}\")\n",
        "\n",
        "# Example of changing the number of estimators and recalculating the ROC-AUC.\n",
        "rf_classifier_modified = RandomForestClassifier(n_estimators = 50, random_state = 42)\n",
        "rf_classifier_modified.fit(X_train, y_train)\n",
        "y_prob_modified = rf_classifier_modified.predict_proba(X_test)[:,1]\n",
        "roc_auc_modified = roc_auc_score(y_test, y_prob_modified)\n",
        "\n",
        "print(f\"Random Forest Classifier ROC-AUC (n_estimators=50): {roc_auc_modified:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "42 Train a Bagging Classifier and evaluate its performance using cross-validation.\n",
        "-import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Generate a sample classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier\n",
        "bagging_clf = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(random_state=42),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Perform cross-validation using KFold\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)  # 5-fold cross-validation\n",
        "\n",
        "# Calculate cross-validation scores (accuracy)\n",
        "cv_scores = cross_val_score(bagging_clf, X, y, cv=kf, scoring='accuracy')\n",
        "\n",
        "# Print the cross-validation scores\n",
        "print(f\"Cross-Validation Scores: {cv_scores}\")\n",
        "print(f\"Mean Cross-Validation Accuracy: {np.mean(cv_scores):.4f}\")\n",
        "print(f\"Standard Deviation of Cross-Validation Accuracy: {np.std(cv_scores):.4f}\")\n",
        "\n",
        "# Example of changing the scoring method.\n",
        "cv_scores_f1 = cross_val_score(bagging_clf, X, y, cv=kf, scoring='f1')\n",
        "\n",
        "print(f\"\\nCross-Validation F1-scores: {cv_scores_f1}\")\n",
        "print(f\"Mean Cross-Validation F1-score: {np.mean(cv_scores_f1):.4f}\")\n",
        "\n",
        "\n",
        "43 Train a Random Forest Classifier and plot the Precision-Recall curve\n",
        "-import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the Random Forest Classifier\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make probability predictions on the test set\n",
        "y_prob = rf_classifier.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\n",
        "\n",
        "# Calculate Precision-Recall curve\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_prob)\n",
        "\n",
        "# Calculate the area under the Precision-Recall curve (AUC-PR)\n",
        "auc_pr = auc(recall, precision)\n",
        "\n",
        "# Plot the Precision-Recall curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, label=f'AUC-PR = {auc_pr:.4f}')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend(loc='lower left')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "#Example of changing RF parameters and replotting.\n",
        "rf_classifier_modified = RandomForestClassifier(n_estimators = 50, max_depth = 5, random_state = 42)\n",
        "rf_classifier_modified.fit(X_train, y_train)\n",
        "y_prob_modified = rf_classifier_modified.predict_proba(X_test)[:, 1]\n",
        "precision_modified, recall_modified, thresholds_modified = precision_recall_curve(y_test, y_prob_modified)\n",
        "auc_pr_modified = auc(recall_modified, precision_modified)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall_modified, precision_modified, label=f'Modified AUC-PR = {auc_pr_modified:.4f}')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve (Modified RF)')\n",
        "plt.legend(loc='lower left')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "44 Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy\n",
        "-import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import StackingClassifier, RandomForestClassifier, LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a sample classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the base estimators\n",
        "estimators = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "    ('lr', LogisticRegression(solver='liblinear', random_state=42))\n",
        "]\n",
        "\n",
        "# Define the final estimator (meta-learner)\n",
        "final_estimator = LogisticRegression(solver='liblinear', random_state=42)\n",
        "\n",
        "# Create the Stacking Classifier\n",
        "stacking_clf = StackingClassifier(estimators=estimators, final_estimator=final_estimator)\n",
        "\n",
        "# Train the Stacking Classifier\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "stacking_y_pred = stacking_clf.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the Stacking Classifier\n",
        "stacking_accuracy = accuracy_score(y_test, stacking_y_pred)\n",
        "\n",
        "print(f\"Stacking Classifier Accuracy: {stacking_accuracy:.4f}\")\n",
        "\n",
        "# Train and evaluate individual models for comparison\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "lr_clf = LogisticRegression(solver='liblinear', random_state=42)\n",
        "\n",
        "rf_clf.fit(X_train, y_train)\n",
        "lr_clf.fit(X_train, y_train)\n",
        "\n",
        "rf_y_pred = rf_clf.predict(X_test)\n",
        "lr_y_pred = lr_clf.predict(X_test)\n",
        "\n",
        "rf_accuracy = accuracy_score(y_test, rf_y_pred)\n",
        "lr_accuracy = accuracy_score(y_test, lr_y_pred)\n",
        "\n",
        "print(f\"Random Forest Accuracy: {rf_accuracy:.4f}\")\n",
        "print(f\"Logistic Regression Accuracy: {lr_accuracy:.4f}\")\n",
        "\n",
        "# Compare the performance\n",
        "if stacking_accuracy > max(rf_accuracy, lr_accuracy):\n",
        "    print(\"\\nStacking Classifier performs better than individual models.\")\n",
        "else:\n",
        "    print(\"\\nStacking Classifier did not outperform the best individual model.\")\n",
        "\n",
        "#Example of changing the final estimator.\n",
        "final_estimator_modified = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "stacking_clf_modified = StackingClassifier(estimators=estimators, final_estimator=final_estimator_modified)\n",
        "stacking_clf_modified.fit(X_train, y_train)\n",
        "stacking_y_pred_modified = stacking_clf_modified.predict(X_test)\n",
        "stacking_accuracy_modified = accuracy_score(y_test, stacking_y_pred_modified)\n",
        "print(f\"Stacking Classifier Accuracy (final estimator = Random Forest): {stacking_accuracy_modified:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "45 Train a Bagging Regressor with different levels of bootstrap samples and compare performance.\n",
        "-import numpy as np\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate a sample regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the range of bootstrap sample ratios to test\n",
        "bootstrap_ratios = [0.5, 0.7, 1.0, 1.2, 1.5]  # 1.0 is the default\n",
        "\n",
        "# Store the MSE for each bootstrap ratio\n",
        "mses = []\n",
        "\n",
        "# Train and evaluate the Bagging Regressor for each bootstrap ratio\n",
        "for ratio in bootstrap_ratios:\n",
        "    bagging_regressor = BaggingRegressor(\n",
        "        base_estimator=DecisionTreeRegressor(random_state=42),\n",
        "        n_estimators=100,\n",
        "        max_samples=ratio,  # Control bootstrap sample size\n",
        "        random_state=42\n",
        "    )\n",
        "    bagging_regressor.fit(X_train, y_train)\n",
        "    y_pred = bagging_regressor.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mses.append(mse)\n",
        "    print(f\"Bootstrap Ratio: {ratio}, MSE: {mse:.4f}\")\n",
        "\n",
        "# Plot the MSE vs. bootstrap ratio\n",
        "plt.plot(bootstrap_ratios, mses, marker='o')\n",
        "plt.title(\"Bagging Regressor MSE vs. Bootstrap Sample Ratio\")\n",
        "plt.xlabel(\"Bootstrap Sample Ratio (max_samples)\")\n",
        "plt.ylabel(\"Mean Squared Error (MSE)\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Find the best bootstrap ratio (lowest MSE)\n",
        "best_ratio = bootstrap_ratios[np.argmin(mses)]\n",
        "best_mse = min(mses)\n",
        "\n",
        "print(f\"\\nBest Bootstrap Ratio: {best_ratio}, Best MSE: {best_mse:.4f}\")\n",
        "\n",
        "#Example of changing base estimator parameters.\n",
        "bagging_regressor_modified = BaggingRegressor(\n",
        "    base_estimator=DecisionTreeRegressor(max_depth=5, random_state=42),\n",
        "    n_estimators=100,\n",
        "    max_samples=1.0,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_regressor_modified.fit(X_train, y_train)\n",
        "y_pred_modified = bagging_regressor_modified.predict(X_test)\n",
        "mse_modified = mean_squared_error(y_test, y_pred_modified)\n",
        "print(f\"Bagging Regressor MSE (max_depth=5): {mse_modified:.4f}\")"
      ],
      "metadata": {
        "id": "mnMu-iCZcgc6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}